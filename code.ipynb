{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors,Word2Vec\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words(\"english\")\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "\n",
    "DATA_DIR_PATH = 'data'\n",
    "DATA_FILE_PATH = os.path.join(DATA_DIR_PATH, 'training.1600000.processed.noemoticon.csv')\n",
    "DATA = pd.read_csv(DATA_FILE_PATH, encoding = \"ISO-8859-1\", names = [\"target\", \"ids\", \"data\", 'flag', \"user\", \"text\"])\n",
    "\n",
    "WORD2VEC_PATH = os.path.join(DATA_DIR_PATH,'GoogleNews-vectors-negative300.bin.gz' )\n",
    "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = {0: \"neg\", 2: 'neu', 4: 'pos'}\n",
    "\n",
    "\n",
    "x_raw = DATA.text\n",
    "y_raw = DATA.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1    is upset that he can't update his Facebook by ...\n",
       "2    @Kenichan I dived many times for the ball. Man...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4    @nationwideclass no, it's not behaving at all....\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_sentence(text):\n",
    "    text = re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def removing_stop_words(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    res = []\n",
    "    for word in words:\n",
    "        if not word in STOPWORDS:\n",
    "            res.append(word)\n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "def stemming_words(text):\n",
    "    words = text.split()\n",
    "    res = []\n",
    "    for word in words:\n",
    "        res.append(STEMMER.stem(word))\n",
    "    return ' '.join(res) \n",
    "\n",
    "\n",
    "def text_pre_process(text):\n",
    "    cleaned = cleaning_sentence(text)\n",
    "    removed = removing_stop_words(cleaned)\n",
    "    stemmed = stemming_words(removed)\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    x_clean = x.apply(lambda item: text_pre_process(item))\n",
    "    return x_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_MAIN = clean(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_MAIN = y_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         awww bummer shoulda got david carr third day\n",
       "1    upset updat facebook text might cri result sch...\n",
       "2      dive mani time ball manag save 50 rest go bound\n",
       "3                      whole bodi feel itchi like fire\n",
       "4                                        behav mad see\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_MAIN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_MAIN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 164 ms, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [_text.split() for _text in X_MAIN]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_MAIN, Y_MAIN, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_seq = 300\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=len_of_seq)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=len_of_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.values.reshape(-1,1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
